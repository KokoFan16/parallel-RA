

\section{Evaluation}
\label{sec:eval}

%There are two goals of this sections 1) evaluate the performance of RA primitive operations Union and Join  2) evaluate the performance of transitive closure, which is a fixed point iteration algorithm.
The goal of this section is to evaluate the performance of our implementation of parallel join, parallel union and parallel transitive closure at scale.
We first individually study the computation and communication components of the RA operations.
Computation is dominated by insertion of tuples and the major challenge faced is that of deduplication.
We first study the efficacy of our btree-based relation container for inserts specifically in the context of deduplication.
All our RA operations involve an all to communication phase, hence, we perform a detailed benchmark of MPI's all to all communication capability.
Finally we benchmark the efficacy of parallel union, parallel join and parallel transitive closure over a wider range of graphs.
%We first study how our data structure performs, and how well are the all to all communication primitives are supported by super computers. More specifically, we study how fast can we insert into our btree based relation class, and how efficiently can it support the task of de-duplication. This is mostly studying the computation aspect of our algorithms. Next, with the all to all tests, we benchmark the communication aspect of our algorithms.


%Radix-hash join and merge-sort join are two of the most popularly used parallel implementations of the inner join operation. Both these algorithms involve partitioning the input data so that they can be efficiently distributed to the participating processes. For example, in the radix-hash approach a tuple is assigned to a process based on the hash output of the column-value on which the join operation is keyed. With this approach, tuples on both relations that share the same hash value are always assigned to the same process. For every tuple in the left-hand side of the join relation is matched against all the tuples of the right-hand side of the join relation. Fast lookup data-structures like hash tables, or radix-trees (TRIE) can be used to organize the tuples within every process. The initial distribution of data using hashing reduces the overall computation overhead by a factor of the number of processes (n).

%More recently (Barthels et al. 2015, 2017), there has been a concerted effort to implement JOIN operations on clusters using an MPI backend. The commonly used radix-hash join and merge-sort join have been re-designed for this purpose. Both these algorithms involve a hash-based partitioning of data so that they are be efficiently distributed to the participating processes and are designed such that inter-process communication is minimized. In both of these implementations one-sided communication is used for transferring data between process. With one-sided communication the initiator of a data transfer request can directly access parts of the remote memory and has full control where the data will be placed. Read and write operations are executed without any involvement of the target machine. This approach of data transfer involves minimal synchronization between particiapting processes and have been shown to scale better that traditional two-sided communication. The implementation of parallel join has shown promising performance numbers; for example, the parallel join algorithm of (Barthels et al. 2017) ran successfully at 4,096 processor cores with up to 4.8 terabytes of input data


\subsection{Dataset and HPC platforms}
\label{sec:datasets}
We have performed our experiments using SuiteSparse Matrix Collection available at ~\cite{}.
SuiteSparse Matrix Collection (formerly known as the University of Florida Sparse Matrix Collection), is a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. 
For our experiments, we chose six graphs representing a wide range in terms of the number of edges. The last three graphs are the largest available graphs.
Transitive closure of a graph with $n$ edges can generate upto $n^2$ edges (a fully connected graph). More generally, the number of edges in the transitive closure of a graph depends on the connectivity of the input graph. We found our third graph with X edges to be the most connected; the transitive closure of the graph generated 260 billion edges, which is 3 terabytes in size.
%-- wide range in terms of number of edges
%-- The transitive closure of third graph generated 260 billion edges, which corresponds to 4 tera bytes of data.

\begin{table}[]
	\begin{tabular}{lllll}
		\begin{tabular}[c]{@{}l@{}}Input graph \\ edge count\end{tabular} & Union & Join & \begin{tabular}[c]{@{}l@{}}Transitive \\ Closure\end{tabular} & Graph name \\
		\hline
		412148                                                            & \checkmark      &      &                                                               &            \\
		2100225                                                           & \checkmark       &      &                                                               &            \\
		6291408                                                           & \checkmark       &      &                                                               &            \\
		59062957                                                          & \checkmark       &      &                                                               &            \\
		136024430                                                         & \checkmark       & \checkmark     &                                                               &            \\
		180292586                                                         & \checkmark       & \checkmark      &                                                               &            \\
		240023949                                                         & \checkmark       &      &                                                               &           
	\end{tabular}
\end{table}


The experiments presented in this work were performed on
Theta at the Argonne Leadership Computing Facility
(ALCF). Theta is a Cray XC30 with a peak
performance of X petaflops, 124, 608 compute cores, 332
TiB of RAM, and 7.5 PiB of online disk storage. We used
Edison Lustre file system (168 GiB/s, 24 I/O servers and 4
Object Storage Targets). 

%Default striping was used with the Lustre file system. Mira system contains 48 racks and 768K cores, and has a theoretical peak performance of 10 petaflops. Each node has 16 cores, with 16 GB of RAM per node. I/O and interprocessor communication travels on a 5D torus network. Every 128 compute nodes has two 2 GB/s bandwidth links to two different I/O nodes, making 4 GB/s bandwidth for I/O at most. I/O nodes are connected to file servers through QDR IB. Mira uses a GPFS file system with 24 PB of capacity and 240 GB/s bandwidth.


\begin{figure*}[t]
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm, 
			clip}]{results/inserts_with_duplicates.pdf}}\hfill%
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm,
			clip}]{results/inserts_with_no_duplicates.pdf}}\hfill%
	\centering
	\caption{Performance evaluation of relation class implemented with btree and unordered map. (left) All tuples are distinct, (right) There are four copies of every tuple being inserted. Relation implemented with btree out-performs the unordered-map implementation.}
	\label{fig:tuple_inserts}
\end{figure*}


\subsection{Btree-Relation container}
\label{sec:relation}

In this section we evaluate the efficacy of our relation container.
We measure performance for two cases: insertions of unique tuples and insertions of tuples with duplicates. With the later set of experiments, every tuple had four duplicates. For both set of experiments, we compare two implementations of the relation class, one with a Btree back-end and the other with an hash map back-end. For the hash map, we used unordered\_map from C++'s standard template library. The results can be seen in Figure~\ref{fig:tuple_inserts}. The X-axis corresponds to the total number of tuples being inserted and the Y-axis is the time taken for the task to finish. We observe that btree-based relation container outperforms the hash map based implementation for insertion of all tuple counts. Furthermore hash map based relation container fails to scale with insertion of very large number of tuples. For example hash based relation takes X seconds to insert Y tuples as opposed to only Z seconds taken by btree based relation. Similar results can be observed for insertion of tuples with duplicates. The btree based relation container successfully dedeuplicate tuples while maintaining high performance.
%To facilitate fast inserts and lookups of tuples, we implemented a relation container.
%We made two implementations of the container one using btrees and other using unordered\_map from the standard template library.
%The relation container is a recursive, nested data-structure. \textbf{More stuff.}
%, for example, for two-tuple we have a btree with keys as the first column-entries, and the value is a new btree which is again
%Deduplication is a major challenge while inserting tuples, for example, join between two relation followed by projection of the common column often yields a lot of duplicate tuples. 


%-- Perform two benchmarks: insertion of tuples without any duplicate and insertion of tupes each with four duplicates.
%-- Relation class implemented with btrees outperforms relation with hashes.
%-- 



\begin{figure*}[t]
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm, 
			clip}]{results/all_to_all_strong.pdf}}\hfill%
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm,
			clip}]{results/all_to_all_weak.pdf}}\hfill%
	\centering
	\caption{Strong (left) and Weak (right) scaling evaluation of MPI\_alltoallv function of MPI.}
	\label{fig:all_to_all}
\end{figure*}


\subsection{MPI\_All\_to\_Allv}
\label{sec:all_to_all}


%All our RA operations require all to all communication.
All to all communication forms the core of all our RA algorithms (\textbf{refer to the algorithms}).
We use MPI's MPI\_Alltoallv function to facilitate all to all data communication.
MPI\_Alltoallv sends data from all to all processes where each process can send a different amount of data by providing displacements for the input and output data. In this section we study both weak and strong scaling characteristics of MPI\_Alltoallv.
For both set of experiments, we varied the number of processes from $2048$ to $32768$. We performed 9 set of weak scaling experiments. For these 9 set of experiments, the amount of data transmitted by each process ($data_{process}$) was varied from 4 megabytes (1st set) to 1024 megabytes (9th set). For a $n$ process run, every process transmits $data_{process}/n$ units of data to every other process. With strong scaling experiments, we performed 6 set of experiments, varying the total amount of data generated across processes ($data_{total}$) from 64 gigabytes (1st set) to 2048 gigabytes (6th set). The amount of data generated by every process is the same, for example for a $n$ process run, and $data_{total}$ total amount of data, every process produces $data_{total}/n$ units of data. A process then transmits $data_{total}/n^2$ units of data to every other process.  The results of both weak and strong scaling experiments can be seen in Figure~\ref{fig:all_to_all}.

For both strong and weak scaling runs, we observe a decline in performance with decreasing workload. For instance, with strong scaling, the 6th set of experiments where total workload is 2048 gigabytes, we observe near perfect scaling when the number of processes is doubled from 2048 (18.5 seconds) to 4096 (7.3 seconds) to 8192 (4.5 seconds). After 8192 processes, although total time continues to come down with increasing process count, we observe that the rate becomes much slower. Furthermore, looking at the 6th set of experiments where total workload is 64 gigabytes we observe relatively poor scaling characteristics across the entire process range. Both the observations can be attributed to reduced per-process workload. With less amount of data to transmit, total time is dominated by initialization costs as opposed to data transmission costs. Similarly for weak scaling experiments as well, when the amount of data exchanged is substantial, we see almost perfect scaling. For example, when the amount of data transmitted by every process is 1024 megabytes, we observe almost perfect scaling, whereas when the amount of data sent by every process is small (4MB) we observe poor scaling.


With the context of communication requirements of RA operations, we find the scaling trends of MPI\_alltoallv to be encouraging. In general for a given workload (graph size for RA operations), there will always be a range of processes that exhibits good all to all scaling characteristics. We will have the challenging task to identify the right process count that balances the tradeoff between computation and communication. As we will see later in section~\ref{sec:tc} with larger per-process load computation cost dominates as opposed to smaller per-process workload where total cost is dominated by communication. 
%As we will ee later the crucial part will be to identify the process count that balances computation and communication in the monst efficient way.

%For every input graph, we need to find the range of process count that exhibits 
%The trends suggests that there is a range of process count suitable for different graphs. There is a window of process count suitable for graphs of differeing sizes. As the number of processes increases the amount of data (tuples) held by every process starts t

%-- perform both weak and strong scaling
%-- configuration
%-- weak scaling with accuracy X percent accuracy for per process load of Y versus Z percent accuracy for per process load of M.
%-- Exhibit poor weak scaling for small load, as opposed to good weak scaling with larger volume of data.
%-- X percent accuracy with strong scaling of X load. This corresponds to Y number of tuples.
%-- Overall very good indication as all to all scales well, aggregate accuracy
%-- different graphs would work better at different scales.

\subsection{Parallel Union}
\label{sec:union}
We use strong scaling to benchmark the performance of parallel union operation. We measure the timing to perform union of all 7 graphs listed in table~\ref{table:graphs}. The number of processes are varied from $64$ to $4,096$. The total number of edges across all $7$ graphs is $664,659,334$ ($~9.9$ gigabytes of data). Union of the seven graphs result in a graph with $424,592,810$ edges, indicating significant overlap between the graphs.

We test the efficacy of both our implementations of parallel union, union\_x and union\_y. Union\_x involves seven epochs of communication and computation (one for every graph) as opposed to union\_y that uses buffering to limit the number of communication and computation epoch to one. For union\_x, we iterate through the seven graphs. The first phase is that of parallel I/O, where all processes access disjoint regions of the file to read equal number of tuples in parallel. Once the tuples are read, every process scans through the tuples and groups them into $nprocs$ (=$hashbuckets$) packets, ready to be sent across the network.
The target process (hash-bucket) of a tuple is decided based on the hash of the key of the tuple. We also perform preliminary deduplication to eliminate duplicate tuples in the input graph. The scan step is followed actual by all to all communication phase where tuples are sent to the appropriate processes (hash buckets). Once tuples arrive at a process, they are inserted into the relation container. This step performs the important task of deduplication of tuples across the graphs. 

With union\_y instead of processing the graphs one after the other, we read all the graphs at once and follow it with one cumulative step of hashing, communication and insertions. This implementation deploys buffering to restrict the number of communication epochs. Performance of both implementation can be seen in Figure~\ref{fig:parallel_union}. For both implementations, we present breakdown of time spent in each of the three phases: parallel I/O, all-to-all communication and insertions. We observe two trends, at all core counts, union\_y outperforms union\_x, this trend can be attributed to an optimized data communication phase. Union\_y leads to communication involving small number of large sized data packets as opposed to union\_x that involves communication with large number of small sized data packets. The other crucial trend is that the union phase scales only till 1024 cores, this can be attributes to increase in communication time at higher core counts. This result is in corroboration with the trend seen in Section~\ref{sec:datasets}. At 2048 and 4096 processes, even though the insertion time gets reduced, the per-process workload becomes very small impeding scalability of the communication phase. 
It can be concluded that for this particular union task of $7$ graphs $1024$ is the ideal degree of parallelism, as that balances the communication and computation task best.

%This step is followed by 

%-- union of 7 graphs from table
%-- union comprises of an io phase followed by communication and then followed by inserts
%-- We compare two union types, one is where we perform io, comm and compute separates, the other is where we first perform io for all and then we bundle all our comm and then we have one phase of compute
%-- scales well upto X cores., this is strong scaling.
%-- faces work load deprecation at low core counts, needs more tuples for union to scale at high core counts
%-- overall a good sign


\subsection{Parallel Join}
\label{sec:join}
Similar to parallel union, we use strong scaling to benchmark the performance of parallel join operation. 
We perform join of two graphs with edges $136,024,430$ and $180,292,586$. The join operation yields a graph with X number of edges.
The number of processes are varied from $64$ to $4,096$. Unlike union, where we maintained only one relation container for all input graphs, here we have separate relation containers for the two input graphs. Once the two relations are initialized across all processes (after parallel I/O, hashing, communication and insertions), we 

-- similar to union join scales well upto X cores, after that there is shortage of work and we do not see proper scaling.
-- communication stops to scale after X core counts. This is because there is lack of work.
-- 


\subsection{Transitive closure}
\label{sec:tc}

Computing the transitive closure of a graph involves repeated join operations until a fixed point is reached. We
use the previously discussed radix-hash join algorithm to distribute the tuples across all processes. The algorithm
can then be roughly divided into four phases: 1) Join 2) network communication 3) insertion 4) checking for a
fixed point. In our join phase every process concurrently computes the join output of the local tuples. In the next
phase every process sends the join output results to the relevant processes. This is a all-to-all communication
phase, which we implemet using MPI’s all to all routines. The next step involves inserting the join output result
received from the network to the output graph’s local partition. In the final step we check if the size of the
output graph changed on any process, if it does then we have not yet reached a fixed point and we continue to
another iteration of these 4 steps.
We performed a set of strong-scaling experiments to compute the transitive closure of graph with 412148
edges—the largest graph in the U. Florida Sparse Matrix set (Davis and Hu 2011). We used the Quartz supercomputer
at the Lawrence Livermore National Laboratory (LLNL). For our runs, we varied the number of processes
from 64 to 2048. A fixed point was attained after 2933 iterations, with the resulting graph containing 1676697415
edges. As can be seen in Figure 1, our approach takes 462 seconds at 64 cores and 235 seconds at 2048 cores, corresponds
to an overall efficiency of 6.25%. We investigated these timings further by plotting the timing breakdown
of by the four major components (join, network communication, join, fixed-point check) of the algorithm. We
observe (see Figure 2) that for all our runs the total time is dominated by computation rather than communication;
insert and join together tended to take up close to 90% of the total time. This is quite an encouraging result
as it shows that we are not bound primarily by the network bandwidth (at these scales and likely moderately
higher ones) and it gives us the opportunity to optimize the computation phase

\begin{figure*}[t]
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm, 
			clip}]{results/TC_1_6Billion.pdf}}\hfill%
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm,
			clip}]{results/TC_1_6Billion_breakdown.pdf}}\hfill%
	\centering
	\caption{Transitive closure of a graph with X edges.}
	\label{fig:tc_small}
\end{figure*}


\begin{figure*}[t]
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm, 
			clip}]{results/TC_260Billion.pdf}}\hfill%
	{\includegraphics[width=.50\textwidth,  trim={0cm 0cm 0cm 0cm,
			clip}]{results/TC_260Billion_breakdown.pdf}}\hfill%
	\centering
	\caption{Transitive closure of a graph with X edges.}
	\label{fig:tc_large}
\end{figure*}