
\section{Related Work}
\label{sec:related}

%% B-trees and data structures
%% B-tree for datalog ~\cite{btree-ppopp}\\
%% Trie for datalog ~\cite{brie-pmam}
%% \\

%% Relational Algebra at scale\\
%% Parallel join ~\cite{Barthels:2017:DJA:3055540.3055545} \\
%% TC with double hashing ~\cite{Cheiney:1990:PST:94362.94445} \\
%% Parallel evaluation of the transitive closure of a database relation (canâ€™t find PDF)\\
%% parallel TC ~\cite{Cacace:1991:OPS:111828.111831}\\


%% Datalog\\
%% automatic generation of indices ~\cite{Subotic:2018:AIS:3282495.3302538}\\
%% souffle ~\cite{10.1007/978-3-319-41540-6_23}
%% souffle ~\cite{Scholz:2016:FLP:2892208.2892226}
%% Doop, program analysis ~\cite{Smaragdakis:2010:UDF:2185923.2185939}


%% BDDs
%% http://www.cs.cmu.edu/afs/cs/academic/class/15745-s05/www/papers/p131-whaley.pdf ~\cite{Whaley:2004:CCP:996893.996859}
%% CUDD: CU decision diagram package (tech report?)
%% var ordering ~\cite{580029}
%% (var ordering) ~\cite{1586251}




The hash join and double-hashing approach is the most commonly used method to parallelize join
operations. This algorithm involve partitioning the input data so that they can be efficiently distributed
to the participating processes. This early approach was first introduced in~\cite{Valduriez:1988:PET:54616.54618}. Much work such as ~\cite{Cheiney:1990:PST:94362.94445} and ~\cite{Cacace:1991:OPS:111828.111831} since then has built on top of this approach. Our approach differs from this method in several respects. We take lessons from state-of-the-art approaches to parallelism on single-node machines and use a nested tree-based structure to encode relations \cite{btree-ppopp,brie-pmam}. In addition, we are the first to address efficient communication and using MPI's all-to-all communication paradigm to permit fixed-point iterations. Finally, ours is the first demonstration of a complete relational algebra application at HPC scale.   

More recently with ~\cite{Barthels:2017:DJA:3055540.3055545}, there has been a concerted effort to implement join operations on clusters using MPI. The commonly used radix-hash join and merge-sort join have been re-designed for this purpose. Both these algorithms involve a hash-based partitioning of data so that they are be efficiently distributed to the participating processes and are designed such that inter-process communication is minimized. In both of these implementations one-sided communication is used for transferring data between process. This implementation only involved scaling to $4,\!096$ nodes and did not address the communication challenges required to implement fixed-point algorithms over RA.
 
There has been some work in the past to scale RA operations on GPUs. For example,
Redfox is a single-node GPU ~\cite{Wu:2014:RFE:2581122.2544166} implementation of RA primitives. While RedFox presents an interesting approach to RA on the GPU, it does not address crucial aspects of the task such as deduplication of tuples generated by joins and unions. Other work such as ~\cite{Martinez-Angeles:2016:RLG:2932241.2932244} and ~\cite{Zinn:2016:GJA:2884045.2884054} have also explored the use of GPUs to scale dedicated RA tasks like the triangle listing problem.

Our implementation heavily relies on all-to-all communication. We use the \texttt{MPI\_Alltoallv} function to transmit data from every process to every other process.
\texttt{MPI\_Alltoallv} is one of the most communication-intensive collective operation used across parallel applications such as CPMD ~\cite{cpmd-web}, NAMD ~\cite{1592872}, LU factorization, Fast Fourier Transform and matrix transpose.
Much research ~\cite{4536141}, ~\cite{642949}, ~\cite{Thakur:2005:OCC:2747766.2747771} has gone into developing scalable implementations of collective operations; most of the existing HPC platforms therefore have scalable implementation of all-to-all operations. 



