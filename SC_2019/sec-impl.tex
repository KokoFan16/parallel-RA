

\section{Implementation}
\label{sec:impl}
%
This section discusses implementing relational algebra efficiently and in parallel. We take a hybrid approach of nesting key-value stores within a hash-table that can be partitioned across multiple cores or nodes. 


\subsection{Representations}

First, we review the two main approaches to encoding relations in a way that is amenable to fast RA algorithms. Unsurprisingly, algorithms for computing join, union, selection, etc, depend greatly on the representation used for relations themselves.

\paragraph{Decision diagrams} Decision diagrams such as binary decision diagrams (BDDs) and zero-supressed binary decision diagrams (ZDDs) are compact representations of relations as decision trees. Each variable (column) storing an integer is broken down into one variable per bit---a relation R(a,b,c) where each column stores a 64bit integer is encoded as a set of binary strings, each 192 bits long.  


\paragraph{Key-value stores} Another approach to encoding relations is to use a hash table, B-tree, prefix tree (trie), or other key-value store.


\paragraph{Hybrid hash-table and b-tree} Our approach is to use an efficient key-value store, but to



\subsection{Hybrid Join}

...

\subsection{Distributed Join}

...


\subsection{Distributed Union}
We present two algorithms for distributed unions, buffered-hash-tree union and naive hash-tree union. As the name suggests buffered-hash-tree union buffers data across all sets that needs to be unioned before performing any communication or insertion related task. Buffered implementation has an extra memory overhead as opposed to the naive implementation where all graphs that needs to be unioned are processed one at a time.

While performing union of $n$ graphs, naive-hash-tree union involves $n$ epochs of communication and computation (one for every graph) as opposed to buffered-hash-tree union that uses buffering to limit the number of communication and computation epochs to one. 

With the naive approach, all processes iterate through the $n$ graphs one by one.
The input graphs can be read from files stored on the disk or can be read from the memory. If the graphs are red from disk, then first phase is that of parallel I/O, where processes access disjoint regions of the file to read equal number of tuples in parallel. Once the tuples are read, every process scans through the tuples and groups them into $nprocs$ (=$hashbuckets$) packets, ready to be sent across the network.
Target process (hash-bucket) of a tuple is computed based on the hash outcome of its key. For instance, target rank of a two column tuple $(a, b)$ would be $hash(a)\%nprocs$. We also perform preliminary deduplication to eliminate duplicate tuples in the input graph. The scan step is followed actual by all to all communication phase where tuples are sent to the appropriate processes (hash buckets). Once tuples arrive at a process, they are inserted into the relation container. This step performs the important task of deduplication of tuples across the graphs. 

With buffered-hash-tree union instead of processing the graphs one after the other, we read all the graphs at once, buffer the tuples and follow it with one cumulative step of hashing, communication and insertion. Both algorithms are presented in X.